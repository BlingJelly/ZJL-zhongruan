今天学习的主要内容是：
一、Hadoop的I/O
Hadoop自带一套原子操作用于数据I/O操作。其中有一些技术比Hadoop本身更常用，如数据完整性和压缩，序列化框架和盘数据结构。
1、数据完整性
HDFS会对写入的所有数据计算校验和，并在读取数据时验证校验和。datanode负责在收到数据后存储该数据及其校验和之前对数据进行验证。datanode的管线中最后一个datanode执行校验。HDFS存储着每一个数据块的复本，因此可以通过数据复本来修复锁坏的数据块。可以用hadoop的命令fs -checksum来检查一个文件的校验和，可用于检查HDFS中两个文件是否具有相同的内容。
2、文件压缩
通用的压缩工具是gzip，是否可切分列表示对应的压缩算法是否支持切分，也就是说，是否可以搜索数据流的任意位置并进一步往下读取数据。可切分压缩格式尤其适合MapReduce。
3、序列化
将结构化对象转化为字节流以便在网络上传输或写到磁盘进行永久存储的过程，反序列化是指将字节流转回结构化对象的逆过程。序列化用于分布式数据处理的两大领域：进程间通信和永久存储。
Hadoop使用的是自己的序列化格式Writable，它紧凑、速度快，但不太容易被Java以外的语言进行扩展和使用。因为Writable是Hadoop的核心(大多数MapReduce程序都会为键和值类型使用它)
二、MapReduce应用开发
1、MapReduce编程流程：
首先写map函数和reduce函数，使用单元测试确保函数的运行符合预期，然后写一个驱动程序来运行作业（可在本地IDE中用一个小数据集进行测试），最后将通过测试的程序放到集群上运行。
2、MapReduce的工作机制
可通过一个简单的方法调用来运行MapReduce作业：Job对象上的submit()。也可调用waitForCompletion()，它用于提交以前没有提交过的作业，并等待它的完成。submit()方法调用封装了大量的处理细节。
